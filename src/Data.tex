\chapter{The discrete data stream ACHTUNG NICHT FERTIG}

This chapter discusses in more detail the discrete data stream used as a basis for the investigation of the bounded history encoding from \cite{borgwardt2015temporalizing}.
It explains where this data comes from, how it is retrieved and how it can be processed.
Finally, it explains how the implementation of the bounded history encoding from \cite{borgwardt2015temporalizing} can query the data.

The publicly accessible data, which is used as a discrete data stream, comes from the online car market AutoScout24 (AS24) \cite{autoscout}.
It provides data on many models of various brands.
The brands and models must be narrowed down so that the data remains manageable but still delivers noticeable results.
It is assumed that it is irrelevant which exact brands and models are collected.
LATER ON IT CAN BE SHOWN WHETHER THIS IS TRUE .
A COMPLETE LIST OF ALL MODELS CAN BE FOUND IN THE APPENDIX REF APPENDIX XY.
To create a real discrete data stream, we collect the data weekly for SEVERAL weeks.
This is the longest possible period for this thesis.
TO CHECK IF THIS AFFECTS THE RESEARCH RESULT, THE RESULT FOR PARTIAL PERIODS WILL BE COMPARED WITH THE RESULTS FOR THE ENTIRE PERIOD .

The data is collected by a scraper implemented in Python, similar to the one presented in \cite{scraperautoscout}.
The scraper uses the modules Beautiful Soup 4 (BS4) \cite{richardson2007beautiful}, pandas \cite{jeff_reback_2020_3715232} and SQLalchemy \cite{sqlalchemy}.

First, it is specified which aspects of an individual car the scraper should cover.
This is necessary because most of the data collected from a website does not contain any information about the car.
AGAIN, IT IS ASSUMED THAT IT IS IRRELEVANT WHICH EXACT DATA IS COLLECTED ABOUT THE CAR.
The most noteworthy attributes, since they are used in examples in this thesis, are "url", "price" and "DELETED".
A COMPLETE LIST OF ALL ASPECTS CAN BE FOUND IN THE APPENDIX REF APPENDIX XY.

Second, the scraper searches AS24 for a specific brand and model and saves the link to each offer the search yields.
This is performed with the BS4 Soupstrainer from \cite{richardson2007beautiful}.
To ensure a consistent order of the offers, they are sorted by age, starting with the newest.

Third, once all links are saved, the scraper searches every offer individually and extracts all the previously determined aspects from the HTML, again using BS4 \cite{richardson2007beautiful}.

Fourth, the collected data is stored in a pandas DataFrame \cite{jeff_reback_2020_3715232} to be loaded further into a SQLite database with SQLalchemy \cite{sqlalchemy}.
The corresponding table is called "autos".
This final step of storing the data in a SQLite database is required because the implementation of the bounded history encoding from \cite{borgwardt2015temporalizing} also operates on a SQLite database.
This enables cross-language interaction between the scraper and the implementation.

On AS24, the search results consist of a maximum of 20 pages, which then themselves contain a maximum of 20 offers.
This maximum of 400 offers per search should not have a major impact on our research outcome.
The number of brands still creates a data set with over 20,000 offers per week, which is expected to be sufficiently large to deliver noticeable results.
Thus, the maximum of 400 offers helps to keep the data manageable.

However, it also presents some challenges.
With some models, offers are added with a high frequency, so the 400 offers found since last week may all be new.
To ensure the scraper searches every offer from last week again, it scans through all previously found offers before searching for new ones.
At the same time, reviewing each offer provides a better reference to check which offers have been deleted.
If the scraper tries to search a deleted offer, an error message is returned.
This provides more certainty than just assuming an offer has been deleted if the search results no longer contain that offer.
The latter could be a consequence of frequently added new offers.
To simplify matters, it is assumed that every deleted offer has been sold.

Although the implementation of the bounded history encoding only requires the data of the current point in time,
instead of replacing the database, we create a new database each time.
Storing all the data brings many advantages for later analysis:
\begin{itemize}
    \item By querying the data one query at a time, it is easier to analyze the impact of each query.
    \item Each step can be repeated to check whether the results are reproducible.
    \item It is possible to query any partial period.
\end{itemize}
Furthermore, the storage of all data allows a comparison of the implementation of the bounded history encoding WITH OTHER
ESTABLISHED APPROACHES.
